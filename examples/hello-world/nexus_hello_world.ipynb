{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEXUS Hello World\n",
    "\n",
    "**Goal:** Get familiar with the NEXUS model\n",
    "\n",
    "This notebook demonstrates the core workflow:\n",
    "1. Installation & Authentication\n",
    "2. The Estimator Loop: Instantiate → Fit → Predict\n",
    "3. Save & Resume patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup\n",
    "\n",
    "First, install the Fundamental SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install\n",
    "# !pip install fundamental-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication\n",
    "\n",
    "Set up your API key. You can either:\n",
    "- Set the `FUNDAMENTAL_API_KEY` environment variable, or\n",
    "- Pass it directly to the client (shown below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fundamental import Fundamental, NEXUSClassifier, NEXUSRegressor\n",
    "import fundamental\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the client\n",
    "client = Fundamental(api_url= 'https://api-demo.fundamental-dev.tech',api_key=\"<api_key>\")\n",
    "fundamental.set_client(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Classification Model\n",
    "\n",
    "### The Estimator Loop: Instantiate → Fit → Predict\n",
    "\n",
    "NEXUS follows the scikit-learn API convention for familiarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classifier\n",
    "# mode=\"quality\" ensures higher accuracy for benchmarks\n",
    "clf = NEXUSClassifier(\n",
    "    mode=\"quality\"  # Options: \"speed\" or \"quality\"\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Fit the model (synchronous training)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"✅ Training Complete!\")\n",
    "print(f\"Model ID: {clf.trained_model_id_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(f\"First 10 predictions: {predictions[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probability estimates\n",
    "probabilities = clf.predict_proba(X_test)\n",
    "\n",
    "print(f\"Probability shape: {probabilities.shape}\")\n",
    "print(f\"First 5 probability estimates:\\n{probabilities[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Regression Model\n",
    "\n",
    "The workflow is identical for regression tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression dataset\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=500,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    noise=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train regressor\n",
    "reg = NEXUSRegressor(mode=\"quality\")\n",
    "\n",
    "print(\"Training regression model...\")\n",
    "reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "print(f\"✅ Regression Training Complete!\")\n",
    "print(f\"Model ID: {reg.trained_model_id_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "reg_predictions = reg.predict(X_test_reg)\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "mse = mean_squared_error(y_test_reg, reg_predictions)\n",
    "r2 = r2_score(y_test_reg, reg_predictions)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save & Resume Models\n",
    "\n",
    "### Option A: Using Model ID\n",
    "\n",
    "Every trained model has a unique `model_id` that you can use to reload it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model ID\n",
    "saved_model_id = clf.trained_model_id_\n",
    "print(f\"Saved Model ID: {saved_model_id}\")\n",
    "\n",
    "# Later, in a new session, reload the model\n",
    "clf_reloaded = NEXUSClassifier()\n",
    "clf_reloaded.trained_model_id_ = saved_model_id\n",
    "\n",
    "# Use the reloaded model\n",
    "reloaded_predictions = clf_reloaded.predict(X_test)\n",
    "print(f\"Reloaded model predictions: {reloaded_predictions[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: List All Models\n",
    "\n",
    "You can retrieve all your trained models from the registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all models\n",
    "models = client.models.list()\n",
    "\n",
    "print(f\"Total models: {len(models)}\")\n",
    "print(\"\\nModel Details:\")\n",
    "for model in models[:5]:  # Show first 5\n",
    "    print(f\"  - ID: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Management\n",
    "\n",
    "### Tagging Models\n",
    "\n",
    "Add metadata to your models for organization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag the model\n",
    "attrs={\n",
    "    \"project\": \"customer_churn\",\n",
    "    \"version\": \"1.0\",\n",
    "    \"stage\": \"development\",\n",
    "    \"author\": \"data-science-team\"\n",
    "}\n",
    "\n",
    "clf_reloaded.set_attributes(attrs\n",
    ")\n",
    "\n",
    "print(\"✅ Model tagged successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting Models\n",
    "\n",
    "Clean up models you no longer need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a specific model (uncomment to use)\n",
    "# client.models.delete(id=saved_model_id)\n",
    "# print(f\"Model {saved_model_id} deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Mode Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models in parallel\n",
    "configs = [\n",
    "    {\"mode\": \"speed\", \"name\": \"fast_model\"},\n",
    "    {\"mode\": \"quality\", \"name\": \"quality_model\"},\n",
    "]\n",
    "\n",
    "jobs = []\n",
    "for config in configs:\n",
    "    clf = NEXUSClassifier(mode=config[\"mode\"])\n",
    "    job = clf.submit_fit_task(X, y)\n",
    "    jobs.append({\"name\": config[\"name\"], \"job\": job, \"clf\": clf})\n",
    "    print(f\"Started {config['name']}: Job ID {job}\")\n",
    "\n",
    "print(f\"\\n{len(jobs)} training jobs running in parallel!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Speed mode\n",
    "clf_speed = NEXUSClassifier(mode=\"speed\")\n",
    "start = time.time()\n",
    "clf_speed.fit(X_train, y_train)\n",
    "speed_time = time.time() - start\n",
    "speed_accuracy = accuracy_score(y_test, clf_speed.predict(X_test))\n",
    "\n",
    "# Quality mode\n",
    "clf_quality = NEXUSClassifier(mode=\"quality\")\n",
    "start = time.time()\n",
    "clf_quality.fit(X_train, y_train)\n",
    "quality_time = time.time() - start\n",
    "quality_accuracy = accuracy_score(y_test, clf_quality.predict(X_test))\n",
    "\n",
    "print(\"\\nMode Comparison:\")\n",
    "print(f\"Speed Mode:   {speed_time:.2f}s | Accuracy: {speed_accuracy:.4f}\")\n",
    "print(f\"Quality Mode: {quality_time:.2f}s | Accuracy: {quality_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting a model, you can compute feature importance to quantify the contribution of each input feature to the model's output:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fundamental import NEXUSClassifier\n",
    "\n",
    "# Fit the model\n",
    "classifier = NEXUSClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "# Get feature importance (waits for computation to complete)\n",
    "feature_importance = classifier.get_feature_importance(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance computation can take a significant amount of time. If you prefer to submit the task and check its status periodically, you can use the asynchronous approach:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Feature importance: {feature_importance}\")\n",
    "\n",
    "# Submit task without waiting\n",
    "task_id = classifier.submit_feature_importance_task(X_test)\n",
    "\n",
    "# Poll status later\n",
    "result = classifier.poll_feature_importance_result(task_id)\n",
    "if result is not None:\n",
    "    print(f\"Feature importance: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
